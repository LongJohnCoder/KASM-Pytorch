{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gensim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.randn((100, 125, 300)) #100 sentences of 125 words and 300 is for glove embeding\n",
    "\n",
    "train_label = torch.ones(100, dtype = torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = torch.randn((20, 125, 300)) #20 sentences, each of 125 words and 300 is for glove embeding\n",
    "\n",
    "train_label = torch.ones(100, dtype = torch.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTMcell(nn.Module):\n",
    "    \n",
    "    def __init__(self, batch_size ,input_size, hidden_size, output_size):\n",
    "        \n",
    "        super(LSTMcell, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        \"\"\"\n",
    "        LSTM cell basic operations\n",
    "        \"\"\"\n",
    "        self.i2ft = nn.Linear(input_size + hidden_size, hidden_size, bias = True)\n",
    "        self.i2it = nn.Linear(input_size + hidden_size, hidden_size, bias = True)\n",
    "        self.i2cdasht = nn.Linear(input_size + hidden_size, hidden_size, bias = True)\n",
    "        self.i2o = nn.Linear(input_size+hidden_size, hidden_size, bias=True)\n",
    "\n",
    "\n",
    "    def forward(self, input, hidden_state, cell_state):\n",
    "        \n",
    "        \"\"\"\n",
    "        input dimension = (batch size X 300); where 300 is dimension used for word embedding\n",
    "        \n",
    "        hidden state dimension = (batch size X 300); where 300 is hidden state dimension as mentioned in the paper\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        combined = torch.cat((input, hidden_state), axis = 1)\n",
    "        \n",
    "        forget_gate = torch.sigmoid(self.i2ft(combined))\n",
    "        i_t = torch.sigmoid(self.i2it(combined))\n",
    "        c_dash = torch.tanh(self.i2cdasht(combined))\n",
    "        cell_state = forget_gate*cell_state + i_t*c_dash\n",
    "        \n",
    "        \"\"\"\n",
    "        IMP: Layer normalization [2] to be performed after the computation of the cell state\n",
    "        \"\"\"\n",
    "        output_state = torch.sigmoid(self.i2o(combined))\n",
    "        hidden_state = output_state*torch.tanh(cell_state)\n",
    "        \n",
    "        \n",
    "        return output_state, hidden_state, cell_state\n",
    "        \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.batch_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMclassifier(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    Classification task on LSTM output\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, batch_size ,input_size, hidden_size, output_size, max_num_of_words):\n",
    "        \n",
    "        super(LSTMclassifier, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = output_size\n",
    "        self.max_num_of_words = max_num_of_words\n",
    "        self.lstm = LSTMcell(batch_size ,input_size ,n_hidden, n_categories)\n",
    "        \n",
    "        \"\"\"\n",
    "        Pooling layer: mean pooling across time \n",
    "                       input dimension: (batch_size X max_num_of_words X 300)\n",
    "                       output dimension: (batch_size X 300)\n",
    "        \n",
    "        \"\"\"\n",
    "        self.pool = nn.AvgPool2d((max_num_of_words,1), stride=1)\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size, bias = True)\n",
    "        self.layer2 = nn.Linear(hidden_size, output_size, bias = True)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \n",
    "        hidden_state = torch.zeros(self.batch_size, self.hidden_size)\n",
    "        cell_state = torch.zeros(self.batch_size, self.hidden_size)\n",
    "        \n",
    "        \"output is concatenation of all time stamp\"\n",
    "        output = torch.zeros((self.batch_size, self.max_num_of_words, 300))\n",
    "        \n",
    "        for i in range(self.max_num_of_words):\n",
    "            output_state, hidden_state, cell_state = self.lstm(input[:,i,:], hidden_state, cell_state)\n",
    "            output[:,i,:] = output_state\n",
    "        \n",
    "        time_avg_output = torch.squeeze(self.pool(output))\n",
    "        linear_layer = torch.sigmoid(self.layer1(time_avg_output))\n",
    "        final_output = torch.sigmoid(self.layer2(linear_layer))\n",
    "        final_output = self.softmax(final_output)\n",
    "        \n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 300\n",
    "input_size = 300\n",
    "n_categories = 5\n",
    "batch_size = 5\n",
    "\n",
    "max_num_of_words = 125 # for this case\n",
    "\n",
    "ann = LSTMclassifier(batch_size ,input_size ,n_hidden, n_categories, max_num_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "learning_rate = 0.005\n",
    "\n",
    "def train(category_tensor, sentence_vec):\n",
    "    \n",
    "    \"\"\"\n",
    "    category tensor dimension: (batch_size X num_of_labels)\n",
    "    sentence_vec dimension: (batch_size X max num of words X 300)\n",
    "    output dimension: (batch_size X num_of_labels)\n",
    "    \"\"\"\n",
    "\n",
    "    ann.zero_grad()\n",
    "    \n",
    "    output = ann(sentence_vec)\n",
    "\n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    for p in ann.parameters():\n",
    "        p.data.add_(-learning_rate, p.grad.data)\n",
    "\n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.592833280563354\n",
      "24.93593692779541\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 2\n",
    "\n",
    "for epoch in range(num_epoch):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for iter in range(0, train_data.size()[0], batch_size):\n",
    "        if iter + batch_size < train_data.size()[0]:\n",
    "            output, loss = train(train_label[iter:(iter+batch_size)], train_data[iter:(iter+batch_size), :, :])\n",
    "        else:\n",
    "            output, loss = train(train_label[iter:(train_data.size()[0])], train_data[iter:(train_data.size()[0]), :, :])\n",
    "#         print(\"loss:\", loss)\n",
    "\n",
    "        running_loss += loss\n",
    "    print(running_loss)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
