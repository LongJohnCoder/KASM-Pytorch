{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kernel2rnn.ipyb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "25-JtbUBXliu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\n",
        "# In[1]:\n",
        "\n",
        "\n",
        "import torch\n",
        "import gensim\n",
        "import numpy as np\n",
        "import pickle as cPickle\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import sys\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "# In[2]:\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LSTMcell(nn.Module):\n",
        "    \n",
        "    def __init__(self,input_size, hidden_size, output_size, cell):\n",
        "        \n",
        "        super(LSTMcell, self).__init__()\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        self.cell = cell\n",
        "        \n",
        "        \"\"\"\n",
        "        LSTM cell basic operations\n",
        "        \"\"\"\n",
        "        self.i2cdasht = nn.Linear(input_size + hidden_size, hidden_size, bias = True)\n",
        "        if self.cell == \"RKM-LSTM\" or self.cell == \"LSTM\":\n",
        "            self.i2ft = nn.Linear(input_size + hidden_size, hidden_size, bias = True)\n",
        "            self.i2it = nn.Linear(input_size + hidden_size, hidden_size, bias = True)\n",
        "            self.i2o = nn.Linear(input_size+hidden_size, hidden_size, bias=True)\n",
        "        if self.cell == \"RKM-CIFG\":\n",
        "            self.i2ft = nn.Linear(input_size + hidden_size, hidden_size, bias = True)\n",
        "            self.i2o = nn.Linear(input_size+hidden_size, hidden_size, bias=True)\n",
        "        if self.cell == \"Linear-kernel-wto\" or self.cell == \"Gated-CNN\":\n",
        "            self.i2o = nn.Linear(input_size+hidden_size, hidden_size, bias=True)\n",
        "        if self.cell == \"Linear-kernel-wto\" or self.cell == \"Linear-kernel\" or self.cell == \"Gated-CNN\" or self.cell == \"CNN\":\n",
        "            self.sigmai = 0.5\n",
        "        if self.cell == \"Linear-kernel-wto\" or self.cell == \"Linear-kernel\":\n",
        "            self.sigmaf = 0.5\n",
        "\n",
        "\n",
        "    def forward(self, input, hidden_state, cell_state):\n",
        "        \n",
        "        \"\"\"\n",
        "        input dimension = (batch size X 300); where 300 is dimension used for word embedding\n",
        "        \n",
        "        hidden state dimension = (batch size X 300); where 300 is hidden state dimension as mentioned in the paper\n",
        "        \n",
        "        \"\"\"\n",
        "\n",
        "        combined = torch.cat((input, hidden_state), axis = 1)\n",
        "        \n",
        "        if self.cell == \"LSTM\" or self.cell == \"RKM-LSTM\" or self.cell == \"RKM-CIFG\":\n",
        "            forget_gate = torch.sigmoid(self.i2ft(combined))\n",
        "        if self.cell == \"LSTM\" or self.cell == \"RKM-LSTM\":\n",
        "            i_t = torch.sigmoid(self.i2it(combined))\n",
        "        c_dash = self.i2cdasht(combined)\n",
        "        \n",
        "        if self.cell == \"LSTM\":\n",
        "            cell_state = forget_gate*cell_state + i_t*torch.tanh(c_dash)\n",
        "        if self.cell == \"RKM-LSTM\":\n",
        "            cell_state = forget_gate*cell_state + i_t*(c_dash)\n",
        "        if self.cell == \"RKM-CIFG\":\n",
        "            cell_state = forget_gate*cell_state + (1 - forget_gate)*c_dash\n",
        "        if self.cell == \"Linear-kernel-wto\" or self.cell == \"Linear-kernel\":\n",
        "            cell_state = self.sigmai*c_dash + self.sigmaf*cell_state\n",
        "        if self.cell == \"Gated-CNN\" or self.cell == \"CNN\":\n",
        "            cell_state = self.sigmai*c_dash\n",
        "#         layer_norm = torch.nn.LayerNorm(cell_state.size()[1:])\n",
        "        cell_state = torch.nn.functional.layer_norm(cell_state, cell_state.size()[1:])\n",
        "        \"\"\"\n",
        "        IMP: Layer normalization [2] to be performed after the computation of the cell state\n",
        "        \"\"\"\n",
        "        if self.cell == \"LSTM\" or self.cell == \"RKM-LSTM\" or self.cell == \"RKM-CIFG\" or self.cell == \"Linear-kernel-wto\" or self.cell == \"Gated-CNN\":\n",
        "            output_state = torch.sigmoid(self.i2o(combined))\n",
        "        if self.cell == \"LSTM\":\n",
        "            hidden_state = output_state*torch.tanh(cell_state)\n",
        "        if self.cell == \"RKM-LSTM\" or self.cell == \"RKM-CIFG\" or self.cell == \"Linear-kernel-wto\" or self.cell == \"Gated-CNN\":\n",
        "            hidden_state = output_state*(cell_state)\n",
        "        if self.cell == \"Linear-kernel\" or self.cell == \"CNN\":\n",
        "            hidden_state = torch.tanh(cell_state)\n",
        "        \n",
        "        \n",
        "        return hidden_state, cell_state\n",
        "\n",
        "\n",
        "# In[3]:\n",
        "\n",
        "\n",
        "class LSTMclassifier(nn.Module):\n",
        "    \n",
        "    \"\"\"\n",
        "    Classification task on LSTM output\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self,input_size, hidden_size, output_size, glove_weights, cell):\n",
        "        \n",
        "        super(LSTMclassifier, self).__init__()\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        self.labels = output_size\n",
        "        \n",
        "        \"\"\"\n",
        "        Glove embeddings initialization\n",
        "        \"\"\"\n",
        "        self.embedding = nn.Embedding.from_pretrained(glove_weights)\n",
        "        \n",
        "        self.lstm = LSTMcell(input_size ,hidden_size, output_size, cell)\n",
        "        \n",
        "        \"\"\"\n",
        "        Pooling layer: mean pooling across time \n",
        "                       pooling layer's input dimension: (batch_size X max_num_of_words X 300)\n",
        "                       pooling layer's output dimension: (batch_size X 300)\n",
        "        \n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        taking intermediate layer size = 100\n",
        "        \"\"\"\n",
        "        self.layer1 = nn.Linear(self.hidden_size, 100, bias = True)\n",
        "        self.layer2 = nn.Linear(100, self.labels, bias = True)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "        \n",
        "    def forward(self, input, max_num_of_words):\n",
        "        \n",
        "        input = (self.embedding(input)).float()\n",
        "        batch_size = input.size()[0]\n",
        "        hidden_state = torch.zeros(batch_size, self.hidden_size)\n",
        "        cell_state = torch.zeros(batch_size, self.hidden_size)\n",
        "        \n",
        "        \"\"\"\n",
        "        output is concatenation of hidden state at all time stamp\n",
        "        \"\"\"\n",
        "        output = torch.zeros((batch_size, max_num_of_words, 300))\n",
        "        if torch.cuda.is_available():\n",
        "            output = output.cuda()\n",
        "            hidden_state = hidden_state.cuda()\n",
        "            cell_state = cell_state.cuda()\n",
        "        \n",
        "        for i in range(max_num_of_words):\n",
        "            hidden_state, cell_state = self.lstm(input[:,i,:], hidden_state, cell_state)\n",
        "            output[:,i,:] = hidden_state\n",
        "        \n",
        "        pool = nn.AvgPool2d((max_num_of_words,1), stride=1)\n",
        "        time_avg_output = torch.squeeze(pool(output))\n",
        "        linear_layer = torch.sigmoid(self.layer1(time_avg_output))\n",
        "        final_output = torch.sigmoid(self.layer2(linear_layer))\n",
        "        final_output = self.softmax(final_output)\n",
        "        \n",
        "        return final_output\n",
        "\n",
        "\n",
        "# In[5]:"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3XNGZOTYPzV",
        "colab_type": "code",
        "outputId": "6cdbc1ba-a35b-442d-b630-41b811f9e60c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbrOEtOuYR9a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dataset(object):\n",
        "    \n",
        "    def load_data(self, dataset):\n",
        "        \n",
        "        self.data = dataset\n",
        "        \n",
        "        if self.data == 'yahoo':\n",
        "            self.loadpath = \"./drive/My Drive/LEAM_dataset/yahoo.p\"\n",
        "            self.embpath = \"./drive/My Drive/LEAM_dataset/yahoo_glove.p\"\n",
        "            self.num_class = 10\n",
        "            self.class_name = ['Society Culture',\n",
        "                'Science Mathematics',\n",
        "                'Health' ,\n",
        "                'Education Reference' ,\n",
        "                'Computers Internet' ,\n",
        "                'Sports' ,\n",
        "                'Business Finance' ,\n",
        "                'Entertainment Music' ,\n",
        "                'Family Relationships' ,\n",
        "                'Politics Government']\n",
        "        elif self.data == 'agnews':\n",
        "            self.loadpath = \"./drive/My Drive/LEAM_dataset/ag_news.p\"\n",
        "            self.embpath = \"./drive/My Drive/LEAM_dataset/ag_news_glove.p\"\n",
        "            self.num_class = 4\n",
        "            self.class_name = ['World',\n",
        "                            'Sports',\n",
        "                            'Business',\n",
        "                            'Science']    \n",
        "        elif self.data == 'dbpedia':\n",
        "            self.loadpath = \"./drive/My Drive/LEAM_dataset/dbpedia.p\"\n",
        "            self.embpath = \"./drive/My Drive/LEAM_dataset/dbpedia_glove.p\"\n",
        "            self.num_class = 14\n",
        "            self.class_name = ['Company',\n",
        "                'Educational Institution',\n",
        "                'Artist',\n",
        "                'Athlete',\n",
        "                'Office Holder',\n",
        "                'Mean Of Transportation',\n",
        "                'Building',\n",
        "                'Natural Place',\n",
        "                'Village',\n",
        "                'Animal',\n",
        "                'Plant',\n",
        "                'Album',\n",
        "                'Film',\n",
        "                'Written Work',\n",
        "                ]\n",
        "        elif self.data == 'yelp_full':\n",
        "            self.loadpath = \"./drive/My Drive/LEAM_dataset/yelp_full.p\"\n",
        "            self.embpath = \"./drive/My Drive/LEAM_dataset/yelp_full_glove.p\"\n",
        "            self.num_class = 5\n",
        "            self.class_name = ['worst',\n",
        "                            'bad',\n",
        "                            'middle',\n",
        "                            'good',\n",
        "                            'best']\n",
        "\n",
        "        x = cPickle.load(open(self.loadpath, \"rb\"), encoding = \"latin1\")\n",
        "        self.train, self.val, self.test = x[0], x[1], x[2]\n",
        "        self.train_lab, self.val_lab, self.test_lab = x[3], x[4], x[5]\n",
        "        self.wordtoix, self.ixtoword = x[6], x[7]\n",
        "        del x\n",
        "        \n",
        "        print(\"load data finished:\", self.data)\n",
        "\n",
        "\n",
        "# In[6]:\n",
        "\n",
        "\n",
        "def eval_model(model, data, label, batch_size):\n",
        "    total_epoch_loss = 0\n",
        "    total_epoch_acc = 0\n",
        "    loss_fn = nn.NLLLoss()\n",
        "    model.eval()\n",
        "    steps = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for iter in range(0, len(data), batch_size):\n",
        "            text = torch.nn.utils.rnn.pad_sequence(data[iter:min((iter+batch_size), len(data))], batch_first = True)\n",
        "            target = label[iter:min((iter+batch_size), len(data))].long()\n",
        "            if torch.cuda.is_available():\n",
        "                text = text.cuda()\n",
        "                target = target.cuda()\n",
        "            prediction = model(text, text.size()[1])\n",
        "            loss = loss_fn(prediction, target)\n",
        "            num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).sum()\n",
        "            acc = 100.0 * num_corrects/min(batch_size, (len(data) - iter))\n",
        "            total_epoch_loss += loss.item()\n",
        "            total_epoch_acc += acc.item()\n",
        "            steps += 1\n",
        "    \n",
        "    return total_epoch_loss/steps, total_epoch_acc/steps\n",
        "\n",
        "\n",
        "# In[7]:\n",
        "\n",
        "\n",
        "def train_model(model, data, label, batch_size, epoch):\n",
        "    total_epoch_loss = 0\n",
        "    total_epoch_acc = 0\n",
        "    \n",
        "    if torch.cuda.is_available():\n",
        "        model.cuda()\n",
        "        \n",
        "    optim = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.01)\n",
        "    steps = 0\n",
        "    loss_fn = nn.NLLLoss()\n",
        "    model.train()\n",
        "    \n",
        "    for iter in range(0, len(data), batch_size):\n",
        "        text = torch.nn.utils.rnn.pad_sequence(data[iter:min((iter+batch_size), len(data))], batch_first = True)\n",
        "        target = label[iter:min((iter+batch_size), len(data))].long()\n",
        "        if torch.cuda.is_available():\n",
        "            text = text.cuda()\n",
        "            target = target.cuda()\n",
        "\n",
        "        optim.zero_grad()\n",
        "        prediction = model(text, text.size()[1])\n",
        "        loss = loss_fn(prediction, target)\n",
        "        num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).sum()\n",
        "        acc = 100.0 * num_corrects/min(batch_size, (len(data) - iter))\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        steps+=1\n",
        "\n",
        "        if steps % 100 == 0:\n",
        "            print (f'Epoch: {epoch+1}, Idx: {iter+1}, Training Loss: {loss.item():.4f}, Training Accuracy: {acc.item(): .2f}%')\n",
        "\n",
        "        total_epoch_loss += loss.item()\n",
        "        total_epoch_acc += acc.item()    \n",
        "    \n",
        "    return total_epoch_loss/steps, total_epoch_acc/steps\n",
        "\n",
        "\n",
        "# In[8]:"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbiYfPhCYsW9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main(params):\n",
        "    \n",
        "    data = Dataset()\n",
        "    data.load_data(params[\"data\"])\n",
        "    from sklearn.utils import shuffle\n",
        "    data.train, data.train_lab = shuffle(data.train, data.train_lab)\n",
        "    data.train = [torch.tensor(x) for x in data.train]\n",
        "    data.test = [torch.tensor(x) for x in data.test]\n",
        "    data.val = [torch.tensor(x) for x in data.val]\n",
        "    data.train_lab = torch.tensor([np.argmax(x) for x in data.train_lab], dtype = torch.int64)\n",
        "    data.test_lab = torch.tensor([np.argmax(x) for x in data.test_lab], dtype = torch.int64)\n",
        "    data.val_lab = torch.tensor([np.argmax(x) for x in data.val_lab], dtype = torch.int64)\n",
        "    \n",
        "    batch_size = 512\n",
        "    if \"batch\" in params:\n",
        "        batch_size = int(params[\"batch\"])\n",
        "    print(\"struture used:\", params[\"cell\"])\n",
        "    n_hidden = 300\n",
        "    input_size = 300 #I guess for n-gram it will be n*300\n",
        "\n",
        "    W_embd = np.array(cPickle.load(open(data.embpath, 'rb'), encoding = \"latin1\"))\n",
        "    W_embd = torch.from_numpy(W_embd)\n",
        "    classifier = LSTMclassifier(input_size ,n_hidden, data.num_class, W_embd, params[\"cell\"])\n",
        "   \n",
        "    num_epoch = 20\n",
        "    for epoch in range(num_epoch):\n",
        "\n",
        "        start_time = time.time()        \n",
        "        train_loss, train_acc = train_model(classifier, data.train, data.train_lab, batch_size, epoch)\n",
        "        end_time = time.time()\n",
        "        elapsed_time = end_time - start_time\n",
        "        hours, rest = divmod(elapsed_time, 3600)\n",
        "        minutes, sec = divmod(rest, 60)\n",
        "        \"\"\"\n",
        "        Change the path to save the model weights and results\n",
        "        \"\"\"\n",
        "        torch.save(classifier, \"./drive/My Drive/neurips/checkpoints/\"+params[\"data\"]+\"/\" + params[\"cell\"] +\"_epoch\"+str(epoch+1)+\".pth\")\n",
        "\n",
        "        val_loss, val_acc = eval_model(classifier, data.val, data.val_lab, batch_size)\n",
        "        test_loss, test_acc = eval_model(classifier, data.test, data.test_lab, batch_size)\n",
        "        print(f'Epoch: {epoch+1:02}, Time(hr,min): {hours, minutes},Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, Val. Loss: {val_loss:3f}, Val. Acc: {val_acc:.2f}%,Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%')\n",
        "#         text_file = open(\"./result/result_\"+ params['cell'] +\"lstm1_\"+data.data + \".txt\", \"a+\")\n",
        "        text_file = open(\"./drive/My Drive/neurips/result/\"+params[\"data\"]+\"/\"+ params[\"cell\"] +\"_\"+ params[\"data\"] +\".txt\", \"a+\")\n",
        "        n = text_file.write(f'Epoch: {epoch+1:02}, Time(hr,min): {hours, minutes},Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, Val. Loss: {val_loss:3f}, Val. Acc: {val_acc:.2f}%,Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%')\n",
        "        m = text_file.write(\"\\n\")\n",
        "        text_file.close()\n",
        "    \n",
        "    #test_loss, test_acc = eval_model(classifier, data.test, data.test_lab, batch_size)\n",
        "    #text_file = open(\"results\" + \".txt\", \"a+\")\n",
        "    #n = text_file.write(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%')\n",
        "    #m = text_file.write(\"\\n\")\n",
        "    #text_file.close()\n",
        "    \n",
        "    print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%')\n",
        "\n",
        "    print('done') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9ptVOBVeOKb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "aad6f96d-5d86-41a4-9e15-96e08ea788f5"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    \n",
        "    d=defaultdict(list)\n",
        "    \"\"\"\n",
        "    pass input as --cell=RKM-LSTM\n",
        "    \"\"\"\n",
        "#     for k, v in ((k.lstrip('-'), v) for k,v in (a.split('=') for a in sys.argv[1:])):\n",
        "#         d[k] = v\n",
        "    d[\"cell\"] = \"CNN\"\n",
        "    d[\"data\"] = \"yelp_full\"\n",
        "    \n",
        "    main(d)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "load data finished: yelp_full\n",
            "struture used: CNN\n",
            "Epoch: 1, Idx: 50689, Training Loss: 1.6040, Training Accuracy:  23.24%\n",
            "Epoch: 1, Idx: 101889, Training Loss: 1.5927, Training Accuracy:  23.63%\n",
            "Epoch: 1, Idx: 153089, Training Loss: 1.6219, Training Accuracy:  24.22%\n",
            "Epoch: 1, Idx: 204289, Training Loss: 1.5766, Training Accuracy:  25.98%\n",
            "Epoch: 1, Idx: 255489, Training Loss: 1.6086, Training Accuracy:  19.53%\n",
            "Epoch: 1, Idx: 306689, Training Loss: 1.6518, Training Accuracy:  20.90%\n",
            "Epoch: 1, Idx: 357889, Training Loss: 1.6431, Training Accuracy:  24.22%\n",
            "Epoch: 1, Idx: 409089, Training Loss: 1.5960, Training Accuracy:  23.24%\n",
            "Epoch: 1, Idx: 460289, Training Loss: 1.5896, Training Accuracy:  20.51%\n",
            "Epoch: 1, Idx: 511489, Training Loss: 1.5824, Training Accuracy:  24.80%\n",
            "Epoch: 1, Idx: 562689, Training Loss: 1.6078, Training Accuracy:  22.27%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTMclassifier. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTMcell. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01, Time(hr,min): (0.0, 17.0),Train Loss: 1.599, Train Acc: 23.31%, Val. Loss: 1.588475, Val. Acc: 23.35%,Test Loss: 1.587, Test Acc: 23.53%\n",
            "Epoch: 2, Idx: 50689, Training Loss: 1.5925, Training Accuracy:  26.56%\n",
            "Epoch: 2, Idx: 101889, Training Loss: 1.5729, Training Accuracy:  24.80%\n",
            "Epoch: 2, Idx: 153089, Training Loss: 1.5729, Training Accuracy:  23.05%\n",
            "Epoch: 2, Idx: 204289, Training Loss: 1.5246, Training Accuracy:  27.73%\n",
            "Epoch: 2, Idx: 255489, Training Loss: 1.5841, Training Accuracy:  27.15%\n",
            "Epoch: 2, Idx: 306689, Training Loss: 1.5688, Training Accuracy:  25.78%\n",
            "Epoch: 2, Idx: 357889, Training Loss: 1.5744, Training Accuracy:  22.85%\n",
            "Epoch: 2, Idx: 409089, Training Loss: 1.5801, Training Accuracy:  20.90%\n",
            "Epoch: 2, Idx: 460289, Training Loss: 1.5772, Training Accuracy:  18.36%\n",
            "Epoch: 2, Idx: 511489, Training Loss: 1.5834, Training Accuracy:  19.34%\n",
            "Epoch: 2, Idx: 562689, Training Loss: 1.6031, Training Accuracy:  21.68%\n",
            "Epoch: 02, Time(hr,min): (0.0, 17.0),Train Loss: 1.579, Train Acc: 22.83%, Val. Loss: 1.604272, Val. Acc: 20.98%,Test Loss: 1.594, Test Acc: 20.00%\n",
            "Epoch: 3, Idx: 50689, Training Loss: 1.4662, Training Accuracy:  37.50%\n",
            "Epoch: 3, Idx: 101889, Training Loss: 1.4197, Training Accuracy:  36.91%\n",
            "Epoch: 3, Idx: 153089, Training Loss: 1.4448, Training Accuracy:  36.33%\n",
            "Epoch: 3, Idx: 204289, Training Loss: 1.4236, Training Accuracy:  35.35%\n",
            "Epoch: 3, Idx: 255489, Training Loss: 1.3718, Training Accuracy:  44.34%\n",
            "Epoch: 3, Idx: 306689, Training Loss: 1.4091, Training Accuracy:  37.89%\n",
            "Epoch: 3, Idx: 357889, Training Loss: 1.3723, Training Accuracy:  43.55%\n",
            "Epoch: 3, Idx: 409089, Training Loss: 1.3704, Training Accuracy:  38.09%\n",
            "Epoch: 3, Idx: 460289, Training Loss: 1.3793, Training Accuracy:  46.48%\n",
            "Epoch: 3, Idx: 511489, Training Loss: 1.3631, Training Accuracy:  44.14%\n",
            "Epoch: 3, Idx: 562689, Training Loss: 1.3671, Training Accuracy:  42.58%\n",
            "Epoch: 03, Time(hr,min): (0.0, 17.0),Train Loss: 1.418, Train Acc: 38.75%, Val. Loss: 1.330248, Val. Acc: 44.79%,Test Loss: 1.354, Test Acc: 43.73%\n",
            "Epoch: 4, Idx: 50689, Training Loss: 1.3692, Training Accuracy:  44.34%\n",
            "Epoch: 4, Idx: 101889, Training Loss: 1.3435, Training Accuracy:  44.92%\n",
            "Epoch: 4, Idx: 153089, Training Loss: 1.3219, Training Accuracy:  50.98%\n",
            "Epoch: 4, Idx: 204289, Training Loss: 1.3271, Training Accuracy:  44.34%\n",
            "Epoch: 4, Idx: 255489, Training Loss: 1.3712, Training Accuracy:  40.23%\n",
            "Epoch: 4, Idx: 306689, Training Loss: 1.3797, Training Accuracy:  42.38%\n",
            "Epoch: 4, Idx: 357889, Training Loss: 1.3537, Training Accuracy:  43.75%\n",
            "Epoch: 4, Idx: 409089, Training Loss: 1.3362, Training Accuracy:  45.12%\n",
            "Epoch: 4, Idx: 460289, Training Loss: 1.3333, Training Accuracy:  43.16%\n",
            "Epoch: 4, Idx: 511489, Training Loss: 1.3011, Training Accuracy:  43.95%\n",
            "Epoch: 4, Idx: 562689, Training Loss: 1.3110, Training Accuracy:  42.77%\n",
            "Epoch: 04, Time(hr,min): (0.0, 17.0),Train Loss: 1.350, Train Acc: 43.87%, Val. Loss: 1.312076, Val. Acc: 41.32%,Test Loss: 1.330, Test Acc: 43.75%\n",
            "Epoch: 5, Idx: 50689, Training Loss: 1.3189, Training Accuracy:  46.68%\n",
            "Epoch: 5, Idx: 101889, Training Loss: 1.3234, Training Accuracy:  47.85%\n",
            "Epoch: 5, Idx: 153089, Training Loss: 1.2858, Training Accuracy:  48.05%\n",
            "Epoch: 5, Idx: 204289, Training Loss: 1.3152, Training Accuracy:  43.55%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0fgmm2ykkp9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}